"""
Zadig workflow manager
"""
import json
from typing import Dict, List, Any, Optional
import aiohttp
import asyncio
import time
import datetime
from ..config.settings import zadig_config, app_config
from ..utils.logger import setup_logger
from ..utils.retry import async_retry
from ..utils.schedule import AdvancedScheduler

logger = setup_logger(__name__)


class ZadigManager:
    """Manager for Zadig workflow operations"""
    
    def __init__(self):
        self.base_url = zadig_config.url
        self.token = zadig_config.token
        self.project_key = zadig_config.project_key
        self.reference_env = app_config.reference_env
        self.headers = {
            "Authorization": f"Bearer {self.token}",
            "Content-Type": "application/json"
        }
    
    @staticmethod # ä½¿ç”¨ classmethod æ–¹ä¾¿è£…é¥°å™¨ç›´æ¥è°ƒç”¨
    @AdvancedScheduler.daily(time_str="03:00", description="è‡ªåŠ¨æ¸…ç†Zadigè¿‡æœŸç¯å¢ƒ")
    def scheduled_cleanup_job():
        """è°ƒåº¦å™¨è§¦å‘çš„å…¥å£å‡½æ•°"""
        manager = ZadigManager() # å®ä¾‹åŒ– manager
        # å› ä¸ºè°ƒåº¦å™¨åœ¨ç‹¬ç«‹çº¿ç¨‹è¿è¡Œï¼Œè¿™é‡Œéœ€è¦å¤„ç†å¼‚æ­¥å¾ªç¯
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(manager.cleanup_expired_environments())
        finally:
            loop.close()


    async def get_environments(self):
        """è·å–é¡¹ç›®ä¸‹æ‰€æœ‰ç¯å¢ƒ"""
        url = f"{self.base_url}/openapi/environments?projectKey={self.project_key}"
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(url, headers=self.headers) as resp:
                    if resp.status != 200:
                        logger.error(f"è·å–ç¯å¢ƒåˆ—è¡¨å¤±è´¥: {resp.status}")
                        return
                    return await resp.json()  # å¿…é¡» await
            except Exception as e:
                logger.error(f"è¯·æ±‚ç¯å¢ƒåˆ—è¡¨å¼‚å¸¸: {e}")
                raise
            
    async def cleanup_expired_environments(self):
        """æ‰§è¡Œå…·ä½“çš„æ¸…ç†é€»è¾‘"""
        logger.info("å¼€å§‹æ‰§è¡Œç¯å¢ƒæ¸…ç†å·¡æ£€...")
        
        # 1. è·å–æ‰€æœ‰ç¯å¢ƒä¿¡æ¯ (è¿™é‡Œå‡è®¾ä½ æœ‰ä¸€ä¸ªè·å–åˆ—è¡¨çš„æ–¹æ³•)
        # envs = await self.get_environments_from_zadig() 
        # è¿™é‡Œç”¨ä½ ä¹‹å‰æä¾›çš„ JSON åˆ—è¡¨é€»è¾‘
        whiteList = ["test17", "test33", "test5", "test50"]
        envList = await self.get_environments()
        expiry_threshold = int(time.time()) - (15 * 24 * 60 * 60)
        
        for env in envList:
            env_name = env.get('env_key') # æ ¹æ®å®é™…å­—æ®µå–å€¼
            update_time = env.get('update_time', 0)
            is_production = env.get('production', False)
            status = env.get("status") 

            # 3. è¿‡æ»¤é€»è¾‘ï¼šéç”Ÿäº§ç¯å¢ƒ ä¸” è¶…è¿‡15å¤©æœªæ›´æ–°
            if not is_production and update_time < expiry_threshold and status != "sleeping" and env_name not in whiteList:
                last_date = datetime.datetime.fromtimestamp(update_time).strftime('%Y-%m-%d')
                logger.info(f"ğŸš© æ£€æµ‹åˆ°è¿‡æœŸç¯å¢ƒ: {env_name} (æœ€åæ›´æ–°æ—¶é—´: {last_date})")

                # ç¬¬äºŒæ­¥ï¼šå¦‚æœå·¥ä½œæµæ›´æ–°æˆåŠŸï¼ˆæˆ–å®¹é”™ï¼‰ï¼Œæ‰§è¡Œç‰©ç†åˆ é™¤ç¯å¢ƒ
                await self._clear_environment(env_name)
        
        logger.info("ç¯å¢ƒæ¸…ç†å·¡æ£€å®Œæˆã€‚")
    
    @async_retry(max_tries=3, exceptions=(aiohttp.ClientError,))
    async def _clear_environment(self, env):
        url = f"{self.base_url}/openapi/environments/{env}?projectKey={self.project_key}&isDelete=true"
        async with aiohttp.ClientSession() as session:
            try:
                async with session.delete(url, headers=self.headers) as resp:
                    if resp.status < 300:
                        logger.info(f"åˆ é™¤ç¯å¢ƒ{env} æˆåŠŸ: {resp.status}")
                        return True
                    logger.error(f"åˆ é™¤ç¯å¢ƒ{env} å¤±è´¥: {resp.status}")
                    return False
            except Exception as e:
                logger.error(f"åˆ é™¤ç¯å¢ƒå¼‚å¸¸: {e}")
                raise

    @async_retry(max_tries=3, exceptions=(aiohttp.ClientError,))
    async def update_workflow_environments(self, action: str, env: str) -> bool:
        """Add or remove environment from workflow parameters"""
        async with aiohttp.ClientSession() as session:
            try:
                # Update backend workflow only
                backend_result = await self._update_backend_workflow(session, action, env)
                
                return backend_result
                
            except Exception as e:
                logger.error(f"Failed to update workflows: {e}")
                raise
    
    def fix_workflow_data(self, data):
    # å°†è„šæœ¬ä¸­çš„å•åæ–œæ æ›¿æ¢ä¸ºåŒåæ–œæ ï¼Œé¿å¼€ YAML è½¬ä¹‰æ£€æŸ¥
        if isinstance(data, dict):
            for k, v in data.items():
                if k == 'script' and isinstance(v, str):
                    # æ ¸å¿ƒä¿®å¤ï¼šå¤„ç† \033 ç­‰è½¬ä¹‰åºåˆ—
                    data[k] = v.replace('\\', '\\\\')
                else:
                    self.fix_workflow_data(v)
        elif isinstance(data, list):
            for item in data:
                self.fix_workflow_data(item)
        return data
    
    async def _update_backend_workflow(self, session: aiohttp.ClientSession, action: str, env: str) -> bool:
        """Update backend workflow (fat-pipelines)"""
        workflow_name = "fat-pipelines"
        
        # Get current workflow configuration
        url = f"{self.base_url}/openapi/workflows/custom/{workflow_name}/detail?projectKey={self.project_key}"
        
        async with session.get(url, headers=self.headers) as response:
            if response.status != 200:
                logger.error(f"Failed to get workflow {workflow_name}: {response.status}")
                return False
            
            data = await response.json()
        
        # Update environment parameter
        params_updated = False
        for param in data.get('params', []):
            if param.get('name') == 'cluster':
                choice_options = param.get('choice_option', [])
                
                if action == 'add' and env not in choice_options:
                    choice_options.append(env)
                    params_updated = True
                    logger.info(f"Added {env} to workflow parameters")
                elif action == 'delete' and env in choice_options:
                    choice_options.remove(env)
                    params_updated = True
                    logger.info(f"Removed {env} from workflow parameters")
                
                param['choice_option'] = choice_options
        
        # Update Apollo configuration stage if deleting
        # if action == 'delete':
        #     stages = data.get('stages', [])
        #     for stage in stages:
        #         if stage.get('name') == 'é…ç½®æ£€æŸ¥':
        #             for job in stage.get('jobs', []):
        #                 if job.get('name') == 'é…ç½®å˜æ›´':
        #                     namespace_list = job.get('spec', {}).get('namespaceList', [])
        #                     job['spec']['namespaceList'] = [
        #                         ns for ns in namespace_list 
        #                         if ns.get('clusterID') != env
        #                     ]
        
        # Save updated workflow
        if params_updated or action == 'delete':
            
            # 2. å°† stages å¯¹è±¡å…ˆè½¬æˆ JSON å­—ç¬¦ä¸²ï¼ŒæŠŠæ‰€æœ‰å•ä¸ªåæ–œæ å˜æˆåŒåæ–œæ 
# è¿™æ ·å‘é€è¿‡å»åï¼ŒZadig å­˜å…¥ YAML çš„å°±æ˜¯åŸå§‹å­—ç¬¦ '\033' è€Œä¸æ˜¯è½¬ä¹‰åçš„æ§åˆ¶ç¬¦
            # fixed_stages = self.fix_workflow_data(stages)
            update_data = {
                "name": workflow_name,
                "display_name": data.get('display_name', 'fat-pipelines'),
                "concurrency_limit": data.get('concurrency_limit', 10),
                "project": self.project_key,
                "params": data.get('params', []),
                "stages": data.get('stages', [])
            }
            
            update_url = f"{self.base_url}/api/aslan/workflow/v4/{workflow_name}?projectName={self.project_key}"
            
            async with session.put(update_url, headers=self.headers, data=json.dumps(update_data, ensure_ascii=False)) as response:
                if response.status >= 200 and response.status < 300:
                    logger.info(f"Updated backend workflow {workflow_name}")
                    return True
                else:
                    error_text = await response.text()
                    logger.error(f"Failed to update workflow: {error_text}")
                    return False
        
        return True
    
    # Frontend workflow update removed - no longer needed
    # The _update_frontend_workflow method has been removed as frontend-service workflow update is no longer required
    
    # @async_retry(max_tries=3, exceptions=(aiohttp.ClientError,))
    # async def add_app_to_workflow(self, app: str, env: str) -> bool:
    #     """Add application configuration to workflow"""
    #     if app == "bo-v1-assets" or app.startswith("backoffice-v1-web"):
    #         # Skip certain apps
    #         return True
        
    #     # Normalize app name
    #     if app.startswith("backoffice-v1-web"):
    #         app = "backoffice-v1-web"
        
    #     workflow_name = "test33"
        
    #     async with aiohttp.ClientSession() as session:
    #         # Get current workflow configuration
    #         url = f"{self.base_url}/openapi/workflows/custom/{workflow_name}/detail?projectKey={self.project_key}"
            
    #         async with session.get(url, headers=self.headers) as response:
    #             if response.status != 200:
    #                 return False
                
    #             data = await response.json()
            
    #         # Find the configuration stage
    #         stages = data.get('stages', [])
    #         updated = False
            
    #         for stage in stages:
    #             if stage.get('name') == 'é…ç½®æ£€æŸ¥':
    #                 for job in stage.get('jobs', []):
    #                     if job.get('name') == 'é…ç½®å˜æ›´':
    #                         namespace_list = job.get('spec', {}).get('namespaceList', [])
                            
    #                         # Check if config already exists
    #                         existing = any(
    #                             ns.get('appID') == app and ns.get('clusterID') == env 
    #                             for ns in namespace_list
    #                         )
                            
    #                         if not existing:
    #                             # Add new configuration
    #                             new_config = {
    #                                 "appID": app,
    #                                 "clusterID": env,
    #                                 "env": "FAT",
    #                                 "kv": [],
    #                                 "namespace": f"web.{app}",
    #                                 "original_config": [],
    #                                 "type": "properties"
    #                             }
    #                             namespace_list.append(new_config)
    #                             updated = True
    #                             logger.info(f"Added {app} configuration for {env}")
            
    #         # Save if updated
    #         if updated:
    #             update_url = f"{self.base_url}/api/aslan/workflow/v4/{workflow_name}?projectName={self.project_key}"
                
    #             async with session.put(
    #                 update_url, 
    #                 headers=self.headers, 
    #                 data=json.dumps(data)
    #             ) as response:
    #                 if response.status >= 200 and response.status < 300:
    #                     logger.info("Updated workflow configuration")
    #                     return True
    #                 else:
    #                     logger.error("Failed to update workflow configuration")
    #                     return False
            
    #         return True